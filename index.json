[{"authors":null,"categories":null,"content":"I’m a PhD student at the Artificial Intelligence and Machine Learning Lab, TU Darmstadt.\nMy main research interests cover a broad range of Machine Learning related topics such as deep models, tractable probabilistic circuits and their applications in computer vision. In specific, I work on bridging the gap between probabilistic circuits and deep neural networks. We want to push the limits of Sum-Product Networks and aim to combine their strengths with the modeling capacity of neural networks.\n","date":1625207298,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625207298,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/steven-lang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/steven-lang/","section":"authors","summary":"I’m a PhD student at the Artificial Intelligence and Machine Learning Lab, TU Darmstadt.\nMy main research interests cover a broad range of Machine Learning related topics such as deep models, tractable probabilistic circuits and their applications in computer vision.","tags":null,"title":"Steven Lang","type":"authors"},{"authors":["Steven Lang"],"categories":["programming"],"content":"A few months ago I had acquired a tablet to read and annotate research papers wherever I want, stopping the waste of printing these in paper form and then keeping the annotated printed versions somewhere in my desk. This led me to a setup in which I store all research papers in some directory structure at ~/papers/ and have this directory be in sync with a synchronization service; currently OneDrive, which has an excellent third-party Linux client.\nMany of the papers I find are hosted on arXiv.org. When I\u0026rsquo;ve got a paper which I\u0026rsquo;d like to read, e.g. Variational Diffusion Models, I first have to manually download the paper into some location. By default, this results in a file named after the arXiv paper id, e.g. 2107.00630.pdf, which is annoying as the id is hard to associate with the actual paper title. Therefore, I usually rename the paper according to its title:\nmv 2107.00630.pdf ~/papers/generative-models/2107.00630v1.Variational_Diffusion_Models.pdf  Now the paper is ready to be automatically synced and I can find it on my tablet device. After repeating the above multiple times, I was especially annoyed by the file renaming necessity. Furthermore, it also happens that I often simply have the arXiv link and know that I want to have this paper on my tablet. I\u0026rsquo;m an avid Linux user and work from the terminal most of the time. Hence, it was natural to write a command-line tool that takes an arXiv link/id, downloads it into a preferred directory, and automatically renames the filename according to the title. This is when I came up with a simple tool called arxiv-downloader, wrapping the arXiv Python wrapper lukasschwab/arxiv.py.\nThis little tool is available on PyPi (pip install arxiv-downloader) and offers the arxiv-downloader script in the command-line:\n$ arxiv-downloader --url https://arxiv.org/abs/2107.00630 --directory ~/papers/generative-models/ Directory /home/steven/papers/generative-models/ does not exist. Create? [y/n] y Starting download of article: \u0026quot;Variational Diffusion Models\u0026quot; (2107.00630) Download finished! Result saved at: /home/steven/papers/generative-models/2107.00630v1.Variational_Diffusion_Models.pdf  This merges the sequence of opening an arXiv link, manually downloading the PDF, renaming this PDF according to the paper title, and moving the file to the final location, into a single command. Although this might only save me about half a minute for every paper, I deeply felt the need to automate these steps. If you\u0026rsquo;re a computer scientist or programmer, I hope you can relate; if not then just call me crazy :-).\nYou can check out all available arguments with arxiv-downloader -h\n$ arxiv-downloader -h usage: arxiv-downloader [-h] [--url URL] [--id ID] [--directory DIRECTORY] [--source] arXiv Paper Downloader. optional arguments: -h, --help show this help message and exit --url URL, -u URL arXiv article URL. --id ID, -i ID arXiv article ID (for https://arxiv.org/abs/2004.13316 this would be 2004.13316). --directory DIRECTORY, -d DIRECTORY Output directory. --source, -s Whether to download the source tar file.  The source code is available at steven-lang/arxiv-downloader. Feel free to contribute, leave feedback, and report issues.\n","date":1625207298,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625207298,"objectID":"2748400eac9b87de0eeeb9cd7a1bc0fd","permalink":"/post/2021-07-02-arxiv-downloader/","publishdate":"2021-07-02T08:28:18+02:00","relpermalink":"/post/2021-07-02-arxiv-downloader/","section":"post","summary":"A few months ago I had acquired a tablet to read and annotate research papers wherever I want, stopping the waste of printing these in paper form and then keeping the annotated printed versions somewhere in my desk.","tags":["arxiv","research"],"title":"arxiv-downloader: The arXiv PDF Command-Line Interface Downloader","type":"post"},{"authors":null,"categories":null,"content":"Master Thesis at the TU Darmstadt in the AIML Lab supervised by Prof. Kristian Kersting and Fabrizio Ventola.\nAbstract Object detection is a fundamental task in computer vision. While substantial progress in the field of axis-aligned bounding-box detection has been made, it suffers from poor performance on oriented objects, resulting in large parts of the bounding box covering non-object related area. Therefore, the recent field of oriented object detection has emerged, generalizing object detection to arbitrary orientations which are commonly found in e.g. aerial view imagery or security camera footage. This enables a tighter fit of bounding boxes, leading to a better separation of bounding boxes especially in cases of dense object distributions. In this work we present DAFNe: a Dense one-stage Anchor-Free deep Network for oriented object detection. As one-stage model, DAFNe performs predictions on a dense grid over the input image, being architecturally simpler in design, as well as easier to optimize than their two-stage alternatives. Being an anchor-free model, DAFNe additionally reduces the prediction complexity by refraining from bounding box anchors which come with many burdens: anchor specifications need more attentive hyper-parameter fine-tuning on a per-dataset basis, increased model size and computational overhead. Moreover, we introduce a novel vectorized corner sorting algorithm to efficiently represent bounding boxes with a canonical corner ordering batch-wise, an orientation-aware center-ness formulation for arbitrary oriented bounding boxes to down-weight low-quality predictions, and a center-to-corner bounding-box prediction strategy that improves object localization performance. Our experiments show that DAFNe outperforms all previous one-stage anchor-free models on DOTA 1.0, to the best of our knowledge. DAFNe improves the prediction accuracy over the previous best results by 4.65% mAP, setting the new state-of-the-art results by achieving 76.95% mAP.\n","date":1620864e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620864e3,"objectID":"4c5695742822dc2fb29d30d3392ae27f","permalink":"/project/master-thesis/","publishdate":"2021-05-13T00:00:00Z","relpermalink":"/project/master-thesis/","section":"project","summary":"Master Thesis at the AIML Lab TU Darmstadt","tags":["Deep Learning","Computer Vision","Oriented Object Detection","Object Detection"],"title":"Oriented Object Detection using a One-Stage Anchor-Free Deep Model","type":"project"},{"authors":null,"categories":null,"content":"Write-up and presentation during the Extended Machine Learning Seminar at TU Darmstadt about the Edward deep probabilistic programming language.\nAbstract Probabilistic programming is the approach to adopt probabilistic models and inference as first class citizens of a programming language. Its main goal is lowering the entry barrier into the field of probabilistic modeling and allow easier and faster prototyping in research.\nIn recent years, a new class of these languages has risen: Deep probabilistic programming languages. Their focus is on unifying probabilistic programming languages with the modeling power, efficiency and composability of deep neural networks in the field of machine learning. We focus on Edward, a Turing-complete deep probabilistic programming language, and compare it to prior, as well as future work. Edward makes probabilistic programming as flexible and computationally efficient as deep learning and allows for rich compositions of probabilistic models and inference procedures.\n","date":1587772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587772800,"objectID":"3f765c0756e8910793644126e697cbf7","permalink":"/project/deep-probabilistic-programming-edward/","publishdate":"2020-04-25T00:00:00Z","relpermalink":"/project/deep-probabilistic-programming-edward/","section":"project","summary":"Extended Machine Learning Seminar at TU Darmstadt","tags":["Probabilistic Modeling","Programming"],"title":"Edward - Deep Probabilistic Programming","type":"project"},{"authors":["Robert Peharz","Steven Lang","Antonio Vergari","Karl Stelzner","Alejandro Molina","Martin Trapp","Guy Van den Broeck","Kristian Kersting","Zoubin Ghahramani"],"categories":null,"content":" \u0026ndash;\u0026gt;\n --  \u0026ndash;\u0026gt;\n -- ","date":1586736e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586736e3,"objectID":"cdc90bfcb95c1e07b001ffa0b9576dd6","permalink":"/publication/einsum-networks/","publishdate":"2020-04-13T00:00:00Z","relpermalink":"/publication/einsum-networks/","section":"publication","summary":"A novel approach to efficient Sum-Product Networks.","tags":["Sum-Product Networks","Probabilistic Circuits","Einsum"],"title":"Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits","type":"publication"},{"authors":null,"categories":null,"content":"Deep Generative Models Course Project at TU Darmstadt.\nGoal The main goal of this project is to learn a conditional GAN that can interpolate between different types of food. We want to achieve fluid transitions between e.g.:\n Burger \u0026lt;-\u0026gt; Pizza  Data  Scraped from google with google-images-download Partly from PizzaGAN  Results Video results can be found here.\nProgressive Growing (Video) \nPizza to Pizza (Video) \nBurger to Burger (Video) \nRandom Latent Space (Video) \nCode Base The code is based on a PyTorch implementation of Improved Training of Wasserstein GAN and a PyTorch implementation of Progressive Growing of GANs\n","date":157896e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":157896e4,"objectID":"ce38a9f39f314e8d04b3eee218417e38","permalink":"/project/food-interpolator/","publishdate":"2020-01-14T00:00:00Z","relpermalink":"/project/food-interpolator/","section":"project","summary":"Project for the Deep Generative Models Course at the TU Darmstadt","tags":["Deep Learning","Deep Generative Models","Generative Adverserial Models","Computer Vision"],"title":"GAN based Food Interpolation","type":"project"},{"authors":["Steven Lang","Felipe Bravo-Marquez","Christopher Beckham","Mark Hall","Eibe Frank"],"categories":null,"content":" \u0026ndash;\u0026gt;\n --  \u0026ndash;\u0026gt;\n -- ","date":1556582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556582400,"objectID":"e143deea86c517b1ecf277359b93bf8b","permalink":"/publication/weka-dl4j/","publishdate":"2019-04-30T00:00:00Z","relpermalink":"/publication/weka-dl4j/","section":"publication","summary":"Deep learning is a branch of machine learning that generates multi-layered representations of data, commonly using artificial neural networks, and has improved the state-of-the-art in various machine learning tasks (e.g., image classification, object detection, speech recognition, and document classification).  However, most popular deep learning frameworks such as TensorFlow and PyTorch require users to write code to apply deep learning.  We present WekaDeeplearning4j,  a  Weka  package  that  makes  deep  learning  accessible through a graphical user interface (GUI). The package uses Deeplearning4j as  its  backend,  provides  GPU  support,  and  enables  GUI-based  training  of deep neural networks such as convolutional and recurrent neural networks. It also provides pre-processing functionality for image and text data.","tags":["Weka","Deep Learning"],"title":"WekaDeeplearning4j: A deep learning package for Weka based on Deeplearning4j","type":"publication"},{"authors":null,"categories":null,"content":"Abstract Monocular depth estimation is concerned with computing a dense depth map from a single image, but faces difficulties especially at object boundaries. Atrous convolutions have been successfully employed to this end in the task of semantic segmentation. In this paper we investigate, whether it is also possible to apply atrous convolutions in unsupervised monocular depth estimation. Specifically, we place an Atrous Spatial Pyramid Pooling block in a convolutional neural network between the encoder and decoder. This block allows for computing feature maps at different spatial scales on top of the encoder output. Our experiments show that atrous convolutions in the proposed setup do not improve depth estimation performance. Furthermore, the necessity of a lower output stride after the encoder, such that an increased receptive field size is even applicable, harms runtime and increases memory consumption. Finally, we show that it is possible to reduce the number of channels after the encoder, which reduces the parameter count without impairing predictions.\nProject on Unsupervised Monocular Depth Estimation Using Atrous Convolutions in the Practical Course for Deep Learning in Computer Vision at TU Darmstadt.\n","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"3f4114cfa321c4a26fdfe2b39dedfb74","permalink":"/project/monodepth/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/project/monodepth/","section":"project","summary":"Practical Course for Deep Learning in Computer Vision at TU Darmstadt","tags":["Computer Vision","Deep Learning","Depth Estimation"],"title":"Unsupervised Monocular Depth Estimation using Atrous Convolutions","type":"project"},{"authors":null,"categories":null,"content":"Developed together with Peter Reutemann at the University of Waikato (New Zealand)\nAlgorithms Unsupervised:\n Principal Component Analysis (PCA) Generalized Least Squares Weighting (GLSW) External Parameter Orthogonalization (EPO) Independent Component Analysis (FastICA)  Supervised:\n Partial Least Squares (PLS1) Simple PLS (SIMPLS) Kernel PLS (KernelPLS) Orthogonal Signal Correction (OPLS) Nonlinear Iterative PLS (NIPALS) Sparse PLS (SparsePLS) Y Gradient based Generalized Least Squares Weighting (YGradientGLSW) Y Gradient based External Parameter Orthogonalization (YGradientEPO) Canonical Correlation Analysis (CCA) Domain Invariant PLS (DIPLS) Variance Constrained PLS Orthogonal Signal Correction (OSC)  \u0026hellip;\n","date":1514851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514851200,"objectID":"6b5715473afe0385c14445c4e5be0d59","permalink":"/project/matrix-algorithms-java/","publishdate":"2018-01-02T00:00:00Z","relpermalink":"/project/matrix-algorithms-java/","section":"project","summary":"Java library of 2-dimensional matrix algorithms","tags":["Data Mining","Open Source Software","Java","Algorithms"],"title":"Java Matrix Algorithms Library","type":"project"},{"authors":null,"categories":null,"content":"Java library of multi-way algorithms.\nFor the documentation, go to: waikato-datamining.github.io/multiway-algorithms.\nDeveloped together with Peter Reutemann at the University of Waikato (New Zealand)\nImplemented Algorithms  Multi-linear PLS Non-Negative Tensor Factorization (NTF) PARAFAC Sequentially Orthogonalized Multilinear PLS (SONPLS) TwoWayPCA  ","date":1512604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512604800,"objectID":"00b6f62e9dbc63601632768ab2cc7c15","permalink":"/project/multiwary-algorithms-java/","publishdate":"2017-12-07T00:00:00Z","relpermalink":"/project/multiwary-algorithms-java/","section":"project","summary":"Java library of multi-way algorithms","tags":["Java","Multi-way","Algorithms","Open Source Software"],"title":"Java Multi-Way Algorithms Library","type":"project"},{"authors":null,"categories":null,"content":"Bachelor Thesis at the JGU Mainz in the Research Group Data-Mining, supervised by Prof. Dr. Stefan Kramer and Dr. Joerg Wicker.\nAbstract Estimating degradation times of molecules, that possibly end up in the environment, is an important aspect for a wide range of industries. They have to take into account regulations that set a legal limit on half-lives of potential harmful molecules used in e.g. drugs or pesticides. This thesis addresses this by building machine learning models for the prediction of environmental pollutant\u0026rsquo;s half-lives. The recently released Eawag-Soil dataset of compounds and environmental conditions is used to explore possible influences on the molecule\u0026rsquo;s persistence. Moreover, a novel neural network architecture is proposed that takes advantage of the bi-relational data scheme.\nThe results indicate that the dataset does either not have enough environmental variables or not enough combinations of compounds with degradation scenarios to explore the influences of the environmental conditions on the half-life times.\n","date":1504224e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224e3,"objectID":"2aaa60622d1352d0070526601e6c49ae","permalink":"/project/bachelor-thesis/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/project/bachelor-thesis/","section":"project","summary":"Bachelor Thesis","tags":["Machine Learning","Deep Learning"],"title":"Machine Learning Based Prediction of Half-Lives of Environmental Pollutants","type":"project"},{"authors":null,"categories":null,"content":"Description Project on the Implementation of the Deep Feature Interpolation for Image Content Changes paper in tensorflow together with Manfred Faldum.\nThe goal of deep feature interpolation is to add a specific target feature to the face of a person. This may be a Mustache or a smiling mouth. The procedure starts with taking k images of people with the feature and k images of people without the feature. These sets (positive/negative) are the input to an - on IMAGENET pretrained - VGG19 network. The output of each image is then fetched at the Relu layers of the third, fourth and fifth convolutional block. This builds a deep feature representation of an image. The deep feature vector of the target feature is then calculated by taking the difference of the mean of the positive and the negative set. To add the target feature to a new image, the target feature vector in the deep feature space will be added to the deep feature representation of this image. The image will then be restored by reverse mapping the deep feature values into the original pixel space, using an optimizer of your choice and the following loss function:\n$$ z = argmin_z \\frac{1}{2}||\\left(\\phi\\left(x\\right) + \\alpha w\\right) - \\phi \\left( z \\right)||_2^2 + \\lambda_{V^\\beta} R_{V^\\beta}\\left(z\\right) $$ $$ R_{V^\\beta}\\left(z\\right) = \\sum_{i,j}\\left( \\left(z_{i,j+1} - z_{i,j}\\right)^2 + \\left(z_{i+1,j} - z_{i,j}\\right)^2 \\right) $$\nWhere $z$ is the new image, $x$ is the input image, $\\phi$ is the mapping into deep feature space, $\\alpha$ is a scalar and $w$ is the deep feature vector of the target feature.\n","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"ab5d71fee830939f94c1c0456b245782","permalink":"/project/deep-feature-interpolation/","publishdate":"2017-02-01T00:00:00Z","relpermalink":"/project/deep-feature-interpolation/","section":"project","summary":"Implementation of the Deep Feature Interpolation for Image Content Changes in TensorFlow","tags":["TensorFlow","Computer Vision"],"title":"Deep Feature Interpolation","type":"project"},{"authors":null,"categories":null,"content":"Abstract (German) Die heutigen POSIX Dateisystem Schnittstellen besitzen keine klare Definition der resultierenden Zustände eines Systemabsturzes. Dies ist problematisch für Anwendungen, welche auf persistenten Speicher zurückgreifen um den ursprünglichen Zustand nach einem solchen Systemabsturz wiederherzustellen. Die Schwierigkeit für Entwickler besteht nun darin, die Reihenfolge und Abhängigkeit zwischen Dateisystem Operationen zu verstehen. Dies kann zu unvorhersehbarem Fehlverhalten, korrupten Anwendungszuständen und im schlimmsten Fall sogar zum Verlust der Daten führen.\nDiese Ausarbeitung beschreibt Absturz-Konsistenz Modelle, analog zu Speicher-Konsistenz Modellen. Sie bestehen aus den folgenden zwei Bestandteilen: Litmus Tests, welche erlaubte und verbotene Verhalten von Dateisystemen zeigen und axiomatische Spezifikationen, welche zulässige Absturzverhalten deklarativ mit einer Menge an Axiomen beschreiben, sowie operationale Spezifikationen, bestehend aus abstrakten Automaten, die relevante Aspekte eines Dateisystem simulieren können.\nDes weiteren wird das Framework FERRITE beschrieben, mit dem man Absturz-Konsistenz Modelle ausarbeiten und gegen echte Dateisysteme validieren kann.\n","date":1472947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472947200,"objectID":"aab5b7604f6e2c57dd6c4477df2e202e","permalink":"/project/bs-seminar/","publishdate":"2016-09-04T00:00:00Z","relpermalink":"/project/bs-seminar/","section":"project","summary":"Talk for the Operating Systems seminar at JGU Mainz","tags":["Operating Systems"],"title":"Specifying and Checking File System Crash-Consistency Models","type":"project"},{"authors":null,"categories":null,"content":"Abstract This report is orientated towards the book Mining of Massive Datasets and describes two novel frameworks for efficient massive data analysis on compute clusters. A short introduction to compute clusters and a distributed file system will build the base for MapReduce. After showing a few examples of applications and optimizations, the disadvantages are listed and lead to the second framework, called Spark, which captures on these drawbacks. The report shows that Spark outperforms the MapReduce implementation Hadoop at iterative tasks with working sets.\n","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"c76fd5e5b1d1aef95b5617d6c2545194","permalink":"/project/dm-seminar/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/project/dm-seminar/","section":"project","summary":"Talk for the Data Mining seminar at JGU Mainz","tags":["Data Mining"],"title":"Mining of Massive Datasets","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]